<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>Ke Ma Homepage</title>
    <!-- Favicon-->
    <link rel="icon" type="image/x-icon" href="assets/img/KM.jpg" />

    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
        type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg bg-secondary text-uppercase fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">Ke Ma (马珂)</a>
            <button
                class="navbar-toggler navbar-toggler-right text-uppercase font-weight-bold bg-primary text-white rounded"
                type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive"
                aria-expanded="false" aria-label="Toggle navigation">
                Menu
                <i class="fas fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#about">About</a></li>
                    <!-- <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger" href="#research">Research</a></li> -->
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#publication">Publication</a></li>
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#projects">Other Projects</a></li>
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#misc">Misc</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <header class="masthead bg-primary text-white text-center" id="about">
        <div class="container d-flex align-items-center flex-column">
            <div class="row">
                <div class="col-md-3">
                    <img class="img-fluid rounded" src="assets/img/kmprofile.jpg" alt="">
                </div>
                <div class="col-md-9 text-left">
                    <p>
                        I am a Ph.D. candidate in the computer vision lab (<a
                            href="https://www3.cs.stonybrook.edu/~cvl/" target="_blank"
                            rel="noopener noreferrer">CVLab</a>) at Stony Brook University.
                        My advisor is Prof. <a href="https://www3.cs.stonybrook.edu/~samaras/" target="_blank"
                            rel="noopener noreferrer">Dimitris Samaras</a>.
                        Before that, I received my Master and Bachelor degree from the School of Electronic and
                        Information Engineering at South China University of Technology (SCUT).
                    </p>
                    <p>
                        My research interest lies in broad computer vision and graphics, including texture analysis, 3D
                        estimation, inverse rendering, image-based rendering.
                    </p>
                    <p>
                        <i class="fas fa-id-card-alt"></i>
                        <a href="assets/Ke_Ma_Curriculum_Vitea.pdf">Curriculum Vitea</a>
                    </p>
                    <p>
                        Email: kemma[AT]cs.stonybrook.edu
                    </p>
                    <p>
                        Address: Stony Brook University, Stony Brook, NY, 11790
                    </p>
                </div>
            </div>
        </div>
    </header>
    <section class="page-section research" id="publication">
        <div class="container">
            <h2 class="page-section-heading text-left text-uppercase text-secondary mb-0">Publication</h2>
            <div class="divider-custom">
                <!-- <div class="divider-custom-icon"><i class="fas fa-circle"></i></div> -->
                <div class="divider-custom-line"></div>
            </div>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/CVMI2021.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>A Joint Spatial and Magnification Based Attention Framework for Large Scale Histopathology
                            Classification</b><br>
                        Jingwei Zhang, Ke Ma, John Van Arnam, Rajarsi Gupta, Joel Saltz, Maria Vakalopoulou, and
                        Dimitris
                        Samaras<br>
                        <i>CVPR Workshops 2021</i>
                    </p>
                    <p class="paper-abstract">
                        Conventional deep learning methods cannot handle the enormous image sizes; instead,
                        they split the image into patches which are exhaustively processed, usually through
                        multi-instance learning
                        approaches.
                        Moreover and especially in histopathology, determining the most appropriate magnification to
                        generate these
                        patches is also exhaustive:
                        a model needs to traverse all the possible magnifications to select the optimal one.
                        We propose a novel spatial and magnification based attention sampling strategy by using a small
                        image
                        thumbnail to determine the spatial location and magnification of informative regions and
                        classify the
                        gigapixel original image based on few patches sampled from these regions.
                        Our experiments on BACH and a subset of the
                        TCGA-PRAD dataset demonstrate that the proposed method runs 2.5 times faster while still
                        maintaining
                        comparable accuracy.</p>
                    <a href="" target="_blank" class="btn btn-outline-primary vert-offset-top-1" role="button">Paper
                        (coming soon)</a>
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/BMVC2020.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>Intrinsic Decomposition of Document Images In-the-Wild</b><br>
                        Sagnik Das, Hassan Ahmed Sial, Ke Ma, Ramon Baldrich, Maria Vanrell, and Dimitris Samaras<br>
                        <i>BMVC 2020</i>
                    </p>
                    <p class="paper-abstract">
                        Performance of automatic document content processing is often affected by artifacts caused by
                        the shape of the paper,
                        non-uniform and diverse lighting conditions.
                        In this paper, we proposed a learning-based architecture that directly estimates document
                        reflectance based on intrinsic image formation which generalizes to challenging illumination
                        conditions.
                        We also created a new dataset, Doc3dShade, improves previous synthetic ones, by adding a large
                        range of realistic and diverse multi-illuminant conditions.
                    </p>
                    <a href="https://www.bmvc2020-conference.com/assets/papers/0906.pdf" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Paper</a>
                    <a href="https://github.com/cvlab-stonybrook/DocIIW" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Project</a>
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/ICCV19.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>DewarpNet: Single-Image Document Unwarping with Stacked 3D and 2D Regression Networks</b><br>
                        Sagnik Das*, Ke Ma*, Zhixin Shu, Dimitris Samaras, and Roy Shilkrot<br>
                        <i>ICCV 2019</i>
                    </p>
                    <p class="paper-abstract">
                        In this
                        work, we propose DewarpNet, a deep-learning approach
                        for document image unwarping from a single image. Our
                        insight is that the 3D geometry of the document not only
                        determines the warping of its texture but also causes the illumination effects. 
                        Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline. 
                        Also, we contribute the largest and most
                        comprehensive dataset for document image unwarping to
                        date – Doc3D. This dataset features multiple ground-truth
                        annotations, including 3D shape, surface normals, UV map,
                        albedo image, etc. Training with Doc3D, we demonstrate
                        state-of-the-art performance for DewarpNet with extensive
                        qualitative and quantitative evaluations.
                    </p>
                    <a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2019/SagnikKe_ICCV19.pdf" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Paper</a>
                    <a href="https://sagniklp.github.io/dewarpnet-webpage/" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Project</a>
                </div>
            </div>
            <br>

            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/CVPR18.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>DocUNet: Document Image Unwarping via A Stacked U-Net</b><br>
                        Ke Ma, Zhixin Shu, Xue Bai, Jue Wang, and Dimitris Samaras<br>
                        <i>CVPR 2018</i>
                    </p>
                    <p class="paper-abstract">
                        Capturing document images is a common way for digitizing and recording physical documents due to the ubiquitousness of mobile cameras. 
                        To make text recognition easier, it is often desirable to digitally flatten a document image when the physical document sheet is folded or curved.
                        In this paper, we develop the first learning-based method to
                        achieve this goal. 
                        We propose a stacked U-Net with intermediate supervision to directly predict the forward mapping from a distorted image to its rectified version. 
                        We create a synthetic dataset with approximately 100 thousand images by warping non-distorted
                        document images.
                        We further create a comprehensive benchmark
                        that covers various real-world conditions. 
                        We evaluate the proposed model quantitatively and qualitatively on
                        the proposed benchmark, and compare it with previous nonlearning-based methods.

                    </p>
                    <a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2018/Ma_CVPR18.pdf" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Paper</a>
                    <a href="https://www3.cs.stonybrook.edu/~cvl/docunet.html" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Project</a>
                </div>
            </div>
            <br>

            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/BMVC17.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>Large-Scale Continual Road Inspection: Visual Infrastructure Assessment in the Wild</b><br>
                        Ke Ma, Minh Hoai, and Dimitris Samaras<br>
                        <i>BMVC 2017</i>
                    </p>
                    <p class="paper-abstract">
                        This work develops a method to inspect the quality of pavement conditions based
                        on images captured from moving vehicles. 
                        Our first contribution in this paper is the development of a method to create a large-scale dataset of pavement images. 
                        Specifically,
                        using map and GPS information, we match the ratings by government inspectors found in
                        public databases to Google Street View images, creating a dataset containing more than
                        700K images from 70K street segments. 
                        We use the dataset to develop a deep-learning
                        method for road assessment, which is based on Convolutional Neural Networks, Fisher
                        Vector encoding, and UnderBagging random forests.

                    </p>
                    <a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2017/Ma2017BMVC.pdf" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Paper</a>
                    <a href="https://vision.cs.stonybrook.edu/~kema/pavement.html" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Project</a>
                </div>
            </div>
            <br>

            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/WACV16.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>Texture classification for rail surface condition evaluation</b><br>
                        Ke Ma, Dimitris Samaras, Michael Petrucci, and Daniel L. Magnus<br>
                        <i>WACV 2016</i>
                    </p>
                    <p class="paper-abstract">
                        Rail surface defects threaten train and passenger safety.
                        Hence rail surfaces must be restored using different processes depending on measurement of the severity of the defects. 
                        In this paper, we propose a new method for automatic
                        classification of rail surface defect severity from images collected by rail inspection vehicles. 
                        It contains 2 components:
                        a rail surface segmentation module, which utilizes structured random forests to generate an edge map and a Generalized Hough Transform to locate the boundaries of the rail
                        surface; and a defect severity classification module, which
                        combines multiple SVM classifiers through a stacked ensemble
                        model. Our experiments
                        on a dataset of 939 images categorized into 8 severity levels
                        achieved 82% accuracy.
                    </p>
                    <a href="https://www3.cs.stonybrook.edu/~cvl/content/papers/2016/rail_condition_eval.pdf" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Paper</a>
                </div>
            </div>
            <br>

            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/pawt.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>The structure–mechanical relationship of palm vascular tissue</b><br>
                        Ningling Wang, Wangyu Liu, Jiale Huang, and Ke Ma<br>
                        <i>Journal of the mechanical behavior of biomedical materials 2014</i>
                    </p>
                    <p class="paper-abstract">
                        To study the structure–mechanical relationship of palm sheath, the cellular structure of the vascular tissue is rebuilt with an
image-based reconstruction method and used to create finite element models. 
The validity
of the models is firstly verified with the results from the tensile tests. Then, the cell walls
inside each of the specific regions (fiber cap, vessel, xylem, etc.) are randomly removed to
obtain virtually imperfect structures. By comparing the magnitudes of performance
degradation in the different imperfect structures, the influences of each region on the
overall mechanical performances of the vascular tissue are discussed.
                    </p>
                </div>
            </div>
            <br>
        </div>
    </section>
    
    <section class="page-section research" id="projects">
        <div class="container">
            <h2 class="page-section-heading text-left text-uppercase text-secondary mb-0">Other Projects</h2>
            <div class="divider-custom">
                <!-- <div class="divider-custom-icon"><i class="fas fa-circle"></i></div> -->
                <div class="divider-custom-line"></div>
            </div>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/medraw.jpg" class="img-thumbnail center-block" style="max-width:100%">
                </div>
                <div class="col-md-9">
                    <p><b>Medraw</b><br>
                    </p>
                    <p class="paper-abstract">
                        Medraw is a web-based interface to facilitate pathologists to annotate regions on histopathology images.
                        The front end is built upon Bootstrap and utilizes <code>canvas</code> tag overlaying a transparent painting layer on the image.
                        The interface is mobile/touch device friendly so pathologists can annotate images by swiping on tablets.
                        The backend is a simple PHP server which can load the previous annotation or save the current annotation.
                        The interface is for internal use only. A snapshot is shown below.
                    </p>
                    <a href="#modal3" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button" data-toggle="modal" data-target="#modal3">Snapshot</a>
                </div>
            </div>
            <br>
            <div class="row vert-offset-top-1 vert-offset-bottom-1">
                <div class="col-md-3">
                    <img src="assets/img/mimir.jpg" class="img-thumbnail center-block" style="max-width:100%">
                    <p style="font-size:0.5rem;">Design from God of War. Drawn by u/Yfelody</p>
                </div>
                <div class="col-md-9">
                    <p><b>Mimir</b><br>
                    </p>
                    <p class="paper-abstract">
                        Mimir is built to better browse over every day's ArXiv Papers.
                        The backend server has a scheduled task downloading all the ArXiv papers that are tagged as computer vision every day.
                        All the pdfs are converted into images on the server as images are easier to load than pdfs for the end devices.
                        On the client end, papers are organized as a list of cards.
                        Each card shows the title, author list, and some meta-information like if the paper is published.
                        A card contains a clickable gallery of images showing the content of the paper.
                        Thus it is convenient to review every day's new papers on mobile devices in a Twitter-reading style. 
                    </p>
                    <a href="https://vision.cs.stonybrook.edu/~kema/misc/mimir.html" target="_blank"
                        class="btn btn-outline-primary vert-offset-top-1" role="button">Mimir</a>
                </div>
            </div>
            <br>
        </div>
    </section>

    <section class="page-section research" id="misc">
        <div class="container">
            <h2 class="page-section-heading text-left text-uppercase text-secondary mb-0">Misc</h2>
            <div class="divider-custom">
                <!-- <div class="divider-custom-icon"><i class="fas fa-circle"></i></div> -->
                <div class="divider-custom-line"></div>
            </div>
            <p>I am a big fan of video games.
            My favorite game is <a href="https://en.wikipedia.org/wiki/%C5%8Ckami">Okami</a> from Clover studio at CAPCOM. 
            The game is originally on PS2 and later remastered on other platforms.
            I beat the game once on PS2 and once on PS4 with the platinum trophy.
            I also like Final Fantasy series.
            I played FF 6, 7, 8, 9, 10, 12, 13, 15 but only beat 7 Core Crisis, 7 Dirge of Cerberus, and 15 (platinum trophy).
            Other games I beat includes Kingdom Hearts 1, 2, 2.8, 3, Uncharted 1, 2, 3, 4, Ratchet & Clank series, Rockman (X) series, and so on so forth.
            Recently I have shifted my gaming time on NS more.
            I just beat Zelda: Breath of the Wild and Link's Awakening, Octpath Traveler.
            </p>

            <p>I also love anime and manga. 
            My favorite character is <a href="https://en.wikipedia.org/wiki/List_of_One-Punch_Man_characters#Saitama">Saitama</a> from One Punch Man by the manga artist <a href="http://galaxyheavyblow.web.fc2.com/">ONE</a> and <a href="https://twitter.com/NEBU_KURO">Yusuke Murata</a>.</p>

            <p>My other hobbies includes <a href="#modal1" data-toggle="modal" data-target="#modal1">painting and drawing, origami</a>, 
                and <a href="#modal2" data-toggle="modal" data-target="#modal2">badminton</a>.</p>
        </div>
    </section>


    <!-- Copyright Section-->
    <div class="copyright py-4 text-center text-white">
        <div class="container">
            <small>
                &copy; All rights reserved
                <!-- This script automatically adds the current year to your website footer-->
                <!-- (credit: https://updateyourfooter.com/)-->
                <script>
                    document.write(new Date().getFullYear());
                </script>
            </small>
        </div>
    </div>
    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes)-->
    <div class="scroll-to-top d-lg-none position-fixed">
        <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top"><i
                class="fa fa-chevron-up"></i></a>
    </div>

    <div class="portfolio-modal modal fade" id="modal1" tabindex="-1" role="dialog" aria-labelledby="modal1Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-12">
                                <!-- Portfolio Modal - Image-->
                                <img class="img-fluid rounded mb-5" src="assets/img/drawing.jpg" alt="..." />
                            </br>
                                <button class="btn btn-primary" data-dismiss="modal">
                                    <i class="fas fa-times fa-fw"></i>
                                    Close
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="modal2" tabindex="-1" role="dialog" aria-labelledby="modal2Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-12">
                                <!-- Portfolio Modal - Image-->
                                <img class="img-fluid rounded mb-5" src="assets/img/badminton.jpg" alt="..." />
                            </br>
                                <button class="btn btn-primary" data-dismiss="modal">
                                    <i class="fas fa-times fa-fw"></i>
                                    Close
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="modal3" tabindex="-1" role="dialog" aria-labelledby="modal3Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-12">
                                <!-- Portfolio Modal - Image-->
                                <img class="img-fluid rounded mb-5" src="assets/img/medrawss.jpg" alt="..." />
                            </br>
                                <button class="btn btn-primary" data-dismiss="modal">
                                    <i class="fas fa-times fa-fw"></i>
                                    Close
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Bootstrap core JS-->
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Third party plugin JS-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script>
    <!-- Contact form JS-->
    <script src="assets/mail/jqBootstrapValidation.js"></script>
    <script src="assets/mail/contact_me.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>